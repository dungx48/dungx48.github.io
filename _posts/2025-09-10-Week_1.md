---
layout: post
author: EnsembleLearningTeams
title: '[M4W1] Random Forest - Adaboost'
---

# 1. Giới thiệu về thuật toán Random forest
## 1.1. Giới thiệu
Random Forest dựa trên việc kết hợp nhiều cây quyết định (Decision Trees) để tạo thành một “khu rừng”. Từ đó lấy kết quả dự đoán bằng cách bỏ phiếu (voting) (nếu phân loại) hoặc lấy trung bình (averaging) (nếu hồi quy).
<!-- <img src="/assets/images/20191122_HOG/pic1.jpg" width="300px" hieght="200px" style="display:block; margin-left:auto; margin-right:auto"> -->


## 1.2. Cách xây dựng cây quyết định
Mục tiêu: Xây dựng một cấu trúc dạng cây để phân loại dữ liệu bằng cách liên tục hỏi các câu hỏi về đặc trưng. Mục đích cuối cùng là tạo ra các nút lá càng “thuần khiết” càng tốt, tức là chỉ chứa các mẫu thuộc về một lớp duy nhất.

Tiêu chí phân tách: Để quyết định thuộc tính nào và ngưỡng nào là tốt nhất để phân tách tại một nút, thuật toán đo lường mức độ impurity (tạm dịch: không thuần khiết) của dữ liệu. Một lần phân tách tốt sẽ làm giảm impurity nhiều nhất.

<!-- <img src="/assets/images/20191122_HOG/pic1.jpg" width="300px" hieght="200px" style="display:block; margin-left:auto; margin-right:auto"> -->

Quy trình xây dựng cây: 
1. Bắt đầu từ gốc: Toàn bộ dữ liệu huấn luyện được đặt vào một nút duy nhất, gọi là nút gốc.
2. Tìm câu hỏi phân tách tốt nhất: Thuật toán lặp qua tất cả đặc trưng và các ngưỡng chia khả dĩ để tìm tiêu chí phân tách giúp giảm độ impurity nhiều nhất. 
3. Phân tách dữ liệu. Dữ liệu tại nút hiện tại được chia thành các nút con mới dựa trên câu trả lời cho câu hỏi tốt nhất đã chọn. 
4. Lặp lại một cách đệ quy. Quy trình từ Bước 2 được lặp cho mỗi nút con. Quá trình này sẽ dừng lại tại một nhánh khi nó trở nên thuần khiết hoặc đạt điều kiện dừng khác (ví dụ: độ sâu tối đa của cây).

## 1.3. Cách hoạt động
Trước khi tìm hiểu thuật toán HOG, tôi sẽ lý giải trước các thuật ngữ được sử dụng:

* **Bước 1: Bootstrapping dữ liệu**:

Từ tập dữ liệu gốc, thuật toán chọn ngẫu nhiên (có hoàn lại) nhiều tập con → mỗi tập con dùng để huấn luyện một cây quyết định khác nhau.
Mục đích: Mỗi cây sẽ học được những quy luật hơi khác nhau, trở thành một “chuyên gia” với góc nhìn riêng, từ đó tạo ra một khu rừng đa dạng.
Kỹ thuật này được gọi là Bagging (Bootstrap Aggregating), bao gồm: 
1. Từ tập huấn luyện gốc D có N điểm dữ liệu, ta tạo ra B tập dữ liệu mới D1, . . . , DB. 
2. Mỗi tập Db (gọi là mẫu bootstrap) được tạo bằng cách lấy mẫu có hoàn lại N lần từ tập D.


* **Bước 2: Xây dựng các Cây quyết định độc lập**:

Khi xây mỗi cây, tại mỗi nút, không dùng toàn bộ đặc trưng (features) mà chỉ chọn ngẫu nhiên một tập con các đặc trưng để chia nhánh. Điều này giúp giảm sự phụ thuộc giữa các cây, làm chúng đa dạng hơn.
Ý tưởng: Tại mỗi bước phân tách của cây, thay vì xem xét tất cả các đặc trưng để tìm ra cách chia tốt nhất, thuật toán chỉ xem xét một tập con ngẫu nhiên các đặc trưng.
Mục đích: Ngăn chặn việc một vài đặc trưng quá mạnh chiếm ưu thế trong tất cả các cây. Điều này buộc các cây phải khám phá các đặc trưng khác, làm chúng trở nên khác biệt và ít tương quan với nhau hơn.
Cụ thể, với mỗi mẫu bootstrap Db, ta xây dựng một cây quyết định Tb. Quá trình xây dựng cây này có hai điểm đặc biệt: 
Lựa chọn đặc trưng ngẫu nhiên: Tại mỗi nút, khi tìm cách phân chia tốt nhất, thuật toán chỉ chọn ngẫu nhiên m đặc trưng (với m < n) và tìm điểm chia tốt nhất chỉ trong số m đặc trưng này. 
Phát triển cây tối đa: Các cây thường được phát triển đến độ sâu tối đa mà không cần pruning (tạm dịch: cắt tỉa). Hiện tượng overfitting của từng cây riêng lẻ sẽ được giảm thiểu nhờ quá trình tổng hợp kết quả ở bước 3.


* **Bước 3: Tổng hợp kết quả để đưa ra dự đoán**: 
Đối với bài toán Phân loại - Lấy phiếu theo đa số (Majority Voting): Nhãn dự đoán cuối cùng là nhãn được nhiều cây “bỏ phiếu” nhất.


# 1. Giới thiệu về thuật toán AdaBoost
## 1.1. Giới thiệu
## 1.2. Cách hoạt động
